{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Item-Item Collaborative Filtering Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this notebook I will present the solution for the *Item-Item CF Programming Assignment*. The assignment comes with a spreadsheet that contains 4 different sheets: Ratings, NormRatings, Matrix and FilterMatrix. The former one has all the ratings the users have given to the films they've watched. The second one is the same one but normalized using the mean rating for each user. The Matrix sheet is the one we should fill to carry out the similarity between each item (in our case films). Finally, the lasta one shrinks the negative distances to zero, that's why it's called filtered.\n",
    "\n",
    "Let's first load all those sheets into different Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# First we load the Ratings sheet and we separate the two extra vectors added for the computation.\n",
    "ratings = pd.read_excel('Assignment 5.xlsx', sheetname='Ratings', index_col=0)\n",
    "l2_norm = ratings.loc['L2'][:-1]\n",
    "ratings = ratings.drop('L2', 0)\n",
    "mean_rating = ratings.loc[:, 'Mean']\n",
    "ratings = ratings.drop('Mean', 1)\n",
    "\n",
    "# Now we will load the normalized ratings.\n",
    "norm_ratings = pd.read_excel('Assignment 5.xlsx', sheetname='NormRatings', index_col=0)\n",
    "norm_l2_norm = norm_ratings.loc['L2']\n",
    "norm_ratings = norm_ratings.drop('L2', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded all the ratings matrixes, we can start working with the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unnormalized Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we are going to start with the Unnormalized approach. What we need to do is to get the similarity between items using the cosine distance with the unnormalized ratings, so let's start by defining a function to do so. Basically, it has to carry out the dot product between the ratings and its transpose and then divide by the multiplication of the l2_norms. Remember that we have to shrink the negative distances to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1: Toy Story (1995)                               1.000000\n",
       "260: Star Wars: Episode IV - A New Hope (1977)    0.747409\n",
       "780: Independence Day (ID4) (1996)                0.690665\n",
       "296: Pulp Fiction (1994)                          0.667846\n",
       "318: Shawshank Redemption, The (1994)             0.667424\n",
       "1265: Groundhog Day (1993)                        0.661016\n",
       "Name: 1: Toy Story (1995), dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_item_similarities(ratings, l2_norm):\n",
    "    similarities = ratings.transpose().fillna(0).dot(ratings.fillna(0)).divide(l2_norm, 0).divide(l2_norm, 1)\n",
    "    similarities = similarities.applymap(lambda x: 0 if x < 0 else x)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "get_item_similarities(ratings, l2_norm).loc['1: Toy Story (1995)'].nlargest(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use such similarities to calculate the recommendations. We are going to use an average of the userâ€™s ratings weighted by similarity to each candidate movie. Remember that we only have to consider those weights for the movies the user has rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527: Schindler's List (1993)                      2.973883\n",
       "1259: Stand by Me (1986)                          2.928801\n",
       "260: Star Wars: Episode IV - A New Hope (1977)    2.922240\n",
       "593: Silence of the Lambs, The (1991)             2.883304\n",
       "2396: Shakespeare in Love (1998)                  2.852131\n",
       "dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_recommendations(ratings, l2_norm, user, n=5):\n",
    "    similarities = get_item_similarities(ratings, l2_norm)\n",
    "    ratings_user = ratings.loc[user]\n",
    "    recommendations = pd.Series(np.zeros(similarities.shape[0]), index=similarities.index)\n",
    "    \n",
    "    for item in ratings.columns:\n",
    "        weights_item = similarities.loc[item]\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        for i in range(len(ratings_user)):\n",
    "            rating = ratings_user[i]\n",
    "            if not np.isnan(rating):\n",
    "                numerator += weights_item[i] * ratings_user[i]\n",
    "                denominator += abs(weights_item[i])\n",
    "        recommendations[item] = numerator/denominator\n",
    "    \n",
    "    return recommendations.nlargest(n)\n",
    "\n",
    "get_recommendations(ratings, l2_norm, 5277)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have seen how to make recommendations using an unnormalized Item-Item CF algorithm. Now we are going to normalize it to see the effect it has on the recommendations.\n",
    "\n",
    "The first thing we need to do is to get the similarities between items. For that we can re-use the *get_similarities* that we previously defined for the unnormalized approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1: Toy Story (1995)                      1.000000\n",
       "34: Babe (1995)                          0.554448\n",
       "356: Forrest Gump (1994)                 0.355780\n",
       "296: Pulp Fiction (1994)                 0.295013\n",
       "318: Shawshank Redemption, The (1994)    0.215975\n",
       "2028: Saving Private Ryan (1998)         0.192799\n",
       "Name: 1: Toy Story (1995), dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_item_similarities(ratings=norm_ratings, l2_norm=norm_l2_norm).loc['1: Toy Story (1995)'].nlargest(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260: Star Wars: Episode IV - A New Hope (1977)    1.326118\n",
       "527: Schindler's List (1993)                      1.246056\n",
       "1259: Stand by Me (1986)                          0.540536\n",
       "593: Silence of the Lambs, The (1991)             0.404379\n",
       "2396: Shakespeare in Love (1998)                  0.362240\n",
       "dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_norm_recommendations(ratings, l2_norm, user, n=5):\n",
    "    similarities = get_item_similarities(ratings, l2_norm)\n",
    "    ratings_user = ratings.loc[user]\n",
    "    recommendations = pd.Series(np.zeros(similarities.shape[0]), index=similarities.index)\n",
    "    \n",
    "    for item in ratings.columns:\n",
    "        weights_item_i = similarities.loc[item]\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        for j in range(len(ratings_user)):\n",
    "            rating = ratings_user[j]\n",
    "            if not np.isnan(rating) and weights_item_i[j] > 0:\n",
    "                weights_item_j = similarities.iloc[j]\n",
    "                numerator += weights_item_i[j] * (ratings_user[j] - weights_item_j.mean())\n",
    "                denominator += abs(weights_item_i[j])\n",
    "        recommendations[item] = numerator/denominator + weights_item_i.mean()\n",
    "    \n",
    "    return recommendations.nlargest(n)\n",
    "\n",
    "get_norm_recommendations(norm_ratings, norm_l2_norm, 5277)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
